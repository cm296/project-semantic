{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import utilsCM\n",
    "import csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load word2sense\n",
    "#already preprocessed in MATLAB so that Wrd2Sns and THINGs overlap --> we have IMAGES, LABELS and SENSES\n",
    "pathtofile = '../code-00-preprocessdataset/'\n",
    "W2S = pd.read_csv(pathtofile + \"ThingsWrd2Sns.txt\", sep=\",\")\n",
    "Y_embeddings = W2S.values[:,1:W2S.shape[1]-1].astype(np.float)\n",
    "\n",
    "tresh_bonf = utilsCM.p2r(.05/Y_embeddings.shape[1], 1470)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keyword = {'ROIpred'}\n",
    "keyword = {'DNNActvtn','ROIpred'}\n",
    "\n",
    "layer =  {'conv_1','conv_5','fc_3'}\n",
    "\n",
    "ROI = {'EVC', 'ObjectROI'}\n",
    "\n",
    "# Sub = [1,2,3,4]\n",
    "Sub = [1,2,3,4]\n",
    "\n",
    "#how many components to keep?\n",
    "Keepncomps = [20]\n",
    "pretrained_val = False\n",
    "\n",
    "datapath = '../../../data-01/'\n",
    "savepath = '../../../data-02/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2S = pd.read_csv(pathtofile + \"ThingsWrd2Sns.txt\", sep=\",\").set_index('item').drop(labels='sense2251', axis=1)\n",
    "\n",
    "##Showing histograms for senses above threshold\n",
    "myDict = {}\n",
    "\n",
    "for ilayer in layer:\n",
    "    \n",
    "    for ikeyword in keyword:        \n",
    "        \n",
    "        for icomps in Keepncomps:\n",
    "            thisPrediction = []\n",
    "            \n",
    "            if ikeyword is 'DNNActvtn':\n",
    "                if not pretrained_val:\n",
    "                    filename = 'PredictSENSES_' + ikeyword + '_'+ ilayer + '_'+ str(icomps) +'PCs_untrained'\n",
    "                    csvfilename = savepath + ikeyword+ \"_\" + ilayer + '_'+ str(icomps) +'PCs_untrained.csv'\n",
    "                else:\n",
    "                    filename = 'PredictSENSES_' + ikeyword + '_'+ ilayer + '_'+ str(icomps) +'PCs'\n",
    "                    csvfilename = savepath + ikeyword+ \"_\" + ilayer + '_'+ str(icomps) +'PCs.csv'\n",
    "                    \n",
    "                thisPrediction = np.load(datapath + filename + '.npy')\n",
    "                \n",
    "                sortedIndeces = np.argsort(thisPrediction)[::-1][0:9] #first 10 senses indeces, from highest to least hgih\n",
    "                \n",
    "                \n",
    "                Subset_W2S = W2S.iloc[:,sortedIndeces]\n",
    "                WeightedWords_rows = (Subset_W2S != 0).any(axis=1)#finds rows of zeros for those tops senses\n",
    "                new_W2S = W2S.loc[WeightedWords_rows] #takes them off from dataset\n",
    "                \n",
    "                ##Goes through each sense, prints out the top ten words and save them in a csv\n",
    "                listOfWeights = []\n",
    "                count = 0  \n",
    "                for i in sortedIndeces:\n",
    "                    count += 1\n",
    "                    listOfWeights.append(\"Top \"+ str(count) + \" Sense \" + str(i))\n",
    "                    listOfWeights.append(new_W2S.iloc[:,i].sort_values(0)[::-1][0:9])\n",
    "                    \n",
    "                    \n",
    "                \n",
    "                #save in csv file\n",
    "                with open(csvfilename, 'w', newline='') as csvfile:\n",
    "                    wr = csv.writer(csvfile, quoting=csv.QUOTE_ALL)\n",
    "                    wr.writerow(listOfWeights)\n",
    "            \n",
    "            \n",
    "            elif ikeyword is 'ROIpred':\n",
    "                \n",
    "                for iROI in ROI:\n",
    "                    thisPrediction = []\n",
    "                    if not pretrained_val:\n",
    "                        filename = 'PredictSENSES_' + ikeyword + '_' +iROI + '_'+ ilayer + '_'+ str(icomps) +'PCs_untrained'\n",
    "                        csvfilename = savepath + ikeyword + \"_\" + iROI +'_' + ilayer + '_'+ str(icomps) +'PCs_untrained.csv'\n",
    "                    else:\n",
    "                        filename = 'PredictSENSES_' + ikeyword + '_' +iROI + '_'+ ilayer + '_'+ str(icomps) +'PCs'\n",
    "                        csvfilename = savepath + ikeyword + \"_\" + iROI +'_' + ilayer + '_'+ str(icomps) +'PCs.csv'\n",
    "\n",
    "                    \n",
    "                    thisPrediction = np.load(datapath + filename + '.npy')\n",
    "                    \n",
    "                    sortedIndeces = np.argsort(thisPrediction)[::-1][0:9] #first 10 senses indeces, from highest to least hgih\n",
    "                    Subset_W2S = W2S.iloc[:,sortedIndeces]#eliminate rows of zeros\n",
    "                    WeightedWords_rows = (Subset_W2S != 0).any(axis=1)\n",
    "                    new_W2S = W2S.loc[WeightedWords_rows] #takes them off from dataset\n",
    "\n",
    "                    listOfWeights = []\n",
    "                    count = 0\n",
    "                    for i in sortedIndeces:\n",
    "                        count += 1\n",
    "                        listOfWeights.append(\"Top \"+ str(count) + \" Sense \" + str(i))\n",
    "                        listOfWeights.append(new_W2S.iloc[:,i].sort_values(0)[::-1][0:9])\n",
    "                                    \n",
    "                    with open(csvfilename, 'w', newline='') as csvfile:\n",
    "                        wr = csv.writer(csvfile, quoting=csv.QUOTE_ALL)\n",
    "                        wr.writerow(listOfWeights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/cmagri1/OneDrive - Johns Hopkins/Project-Word2Sense/Code-Python/semantic-code/cm/code-01-analysis'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
